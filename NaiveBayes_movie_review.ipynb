{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Excercise\n",
    "\n",
    "The probability of A given that B is true equals the probability of B given that A is true times the probability of A being true, divided by the probability of B being true.”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "days = [[\"ran\", \"was tired\"], [\"ran\", \"was not tired\"], [\"didn't run\", \"was tired\"], [\"ran\", \"was tired\"], [\"didn't run\", \"was not tired\"], [\"ran\", \"was not tired\"], [\"ran\", \"was tired\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['ran', 'was tired'],\n",
       " ['ran', 'was not tired'],\n",
       " [\"didn't run\", 'was tired'],\n",
       " ['ran', 'was tired'],\n",
       " [\"didn't run\", 'was not tired'],\n",
       " ['ran', 'was not tired'],\n",
       " ['ran', 'was tired']]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This is P(A).\n",
    "prob_tired = len([d for d in days if d[1] == \"was tired\"]) / len(days)\n",
    "# This is P(B).\n",
    "prob_ran = len([d for d in days if d[0] == \"ran\"]) / len(days)\n",
    "# This is P(B|A).\n",
    "prob_ran_given_tired = len([d for d in days if d[0] == \"ran\" and d[1] == \"was tired\"]) / len([d for d in days if d[1] == \"was tired\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of being tired given that you ran: 0.6\n"
     ]
    }
   ],
   "source": [
    "# Now we can calculate P(A|B).\n",
    "prob_tired_given_ran = (prob_ran_given_tired * prob_tired) / prob_ran\n",
    "\n",
    "print(\"Probability of being tired given that you ran: {0}\".format(prob_tired_given_ran))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes \n",
    "\n",
    "The probability that classification y is correct given the features x1x1, x2x2, and so on equals the probability of y times the product of each x feature given y, divided by the probability of the x features”.\n",
    "\n",
    "To find the “right” classification, we just find out which classification P(y∣x1,…,xn)) has the highest probability with the formula."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Here's our data, but with \"woke up early\" or \"didn't wake up early\" added.\n",
    "days = [[\"ran\", \"was tired\", \"woke up early\"], [\"ran\", \"was not tired\", \"didn't wake up early\"], [\"didn't run\", \"was tired\", \"woke up early\"], [\"ran\", \"was tired\", \"didn't wake up early\"], [\"didn't run\", \"was tired\", \"woke up early\"], [\"ran\", \"was not tired\", \"didn't wake up early\"], [\"ran\", \"was tired\", \"woke up early\"]]\n",
    "# We're trying to predict whether or not the person was tired on this day.\n",
    "new_day = [\"ran\", \"didn't wake up early\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['ran', 'was tired', 'woke up early'],\n",
       " ['ran', 'was not tired', \"didn't wake up early\"],\n",
       " [\"didn't run\", 'was tired', 'woke up early'],\n",
       " ['ran', 'was tired', \"didn't wake up early\"],\n",
       " [\"didn't run\", 'was tired', 'woke up early'],\n",
       " ['ran', 'was not tired', \"didn't wake up early\"],\n",
       " ['ran', 'was tired', 'woke up early']]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# P(tired)\n",
    "def cal_y_prob(y_label,days):\n",
    "    y_prob = len([d for d in days if d[1] == y_label]) / len(days)\n",
    "    return y_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#x1 is ran given tired\n",
    "# P(tired and ran)/P(tired)\n",
    "def cal_x1_given_y_prob(x1_label,y_label,days):    \n",
    "    x1_given_y_prob = len([\n",
    "            d for d in days if d[0] == x1_label and d for d in days if d[1] == y_label\n",
    "        ]) / len([\n",
    "            d for d in days if d[1] == y_label\n",
    "        ])\n",
    "    return x1_given_y_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#x1 is woke given tired\n",
    "def cal_x2_given_y_prob(x2_label,y_label,days):    \n",
    "    x2_given_y_prob = len([\n",
    "            d for d in days if d[2] == x2_label and d for d in days if d[1] == y_label\n",
    "        ]) / len([\n",
    "            d for d in days if d[1] == y_label\n",
    "        ])\n",
    "    return x2_given_y_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "denominator = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final classification for new day: was tired. Tired probability: 0.10204081632653061. Not tired probability: 0.054421768707482984.\n"
     ]
    }
   ],
   "source": [
    "def calc_y_probability(y_label, days):\n",
    "  return len([d for d in days if d[1] == y_label]) / len(days)\n",
    "\n",
    "def calc_ran_probability_given_y(ran_label, y_label, days):\n",
    "  return len([d for d in days if d[1] == y_label and d[0] == ran_label]) / len(days)\n",
    "\n",
    "def calc_woke_early_probability_given_y(woke_label, y_label, days):\n",
    "  return len([d for d in days if d[1] == y_label and d[2] == woke_label]) / len(days)\n",
    "\n",
    "denominator = len([d for d in days if d[0] == new_day[0] and d[2] == new_day[1]]) / len(days)\n",
    "# Plug all the values into our formula.  Multiply the class (y) probability, and the probability of the x-values occuring given that class.\n",
    "prob_tired = (calc_y_probability(\"was tired\", days) * calc_ran_probability_given_y(new_day[0], \"was tired\", days) * calc_woke_early_probability_given_y(new_day[1], \"was tired\", days)) / denominator\n",
    "\n",
    "prob_not_tired = (calc_y_probability(\"was not tired\", days) * calc_ran_probability_given_y(new_day[0], \"was not tired\", days) * calc_woke_early_probability_given_y(new_day[1], \"was not tired\", days)) / denominator\n",
    "\n",
    "# Make a classification decision based on the probabilities.\n",
    "classification = \"was tired\"\n",
    "if prob_not_tired > prob_tired:\n",
    "  classification = \"was not tired\"\n",
    "print(\"Final classification for new day: {0}. Tired probability: {1}. Not tired probability: {2}.\".format(classification, prob_tired, prob_not_tired))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7142857142857143"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_y_probability(\"was tired\",days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((5/7)*(3/25))/(3/7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.10204081632653061"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((5/7)*(3/49))/(3/7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.39999999999999997"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(0.36 * (2/5)) / 0.36"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Based on Movie review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/Users/dantili/anaconda/lib/python3.5/site-packages/sklearn']\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "print(sklearn.__path__)\n",
    "import pandas as pd  \n",
    "import os\n",
    "import nltk # (Natural Language Toolkit)\n",
    "from nltk.corpus import stopwords # Import the stop word list\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import word2vec #pip install word2vec\n",
    "from bs4 import BeautifulSoup  \n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"labeledTrainData.tsv\", header=0, \\\n",
    "                    delimiter=\"\\t\", quoting=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def each_review_to_words(raw_file):   \n",
    "    # 1.remove html signs\n",
    "    remove_html = BeautifulSoup(raw_file,\"lxml\")\n",
    "    # 2.find pattern and replace it with a space\n",
    "    letter_only = re.sub(\"[^a-zA-Z]\",           # The pattern to search for\n",
    "                      \" \",                   # The pattern to replace it with\n",
    "                      remove_html.get_text())\n",
    "    # 3.Tokenization: convert our reviews to lower case and split them into individual words\n",
    "    lower_case = letter_only.lower() \n",
    "    words = lower_case.split() \n",
    "    # 4.remove stop words\n",
    "    # searching set is faster\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "    # remove all things that are 1 or 2 characters long \n",
    "    words = [w for w in words if len(w)>1]\n",
    "    return words   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_all_reviews(raw):\n",
    "    # Get the number of reviews based on the dataframe column size\n",
    "    num_reviews = raw.size\n",
    "    # Initialize an empty list to hold the clean reviews\n",
    "    clean_train_reviews = []\n",
    "    # Loop over each review; create an index i that goes from 0 to the length\n",
    "    # of the movie review list \n",
    "    for i in range( 0, num_reviews ):\n",
    "    # Call our function for each one, and add the result to the list of\n",
    "    # Clean reviews\n",
    "        clean_train_reviews.append(each_review_to_words(raw[i]))\n",
    "    return clean_train_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'clean_train_reviews' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-061ed39e867e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mclean_train_reviews\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'clean_train_reviews' is not defined"
     ]
    }
   ],
   "source": [
    "clean_train_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "review_list = process_all_reviews(train[\"review\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<25000x74849 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 3445861 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(train['review'])\n",
    "X_train_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 37895)\t1\n",
      "  (0, 31595)\t1\n",
      "  (0, 38587)\t1\n",
      "  (0, 60074)\t1\n",
      "  (0, 63831)\t1\n",
      "  (0, 23376)\t1\n",
      "  (0, 21162)\t1\n",
      "  (0, 23515)\t1\n",
      "  (0, 19676)\t1\n",
      "  (0, 12932)\t1\n",
      "  (0, 6648)\t1\n",
      "  (0, 18336)\t1\n",
      "  (0, 6334)\t1\n",
      "  (0, 10357)\t1\n",
      "  (0, 19563)\t1\n",
      "  (0, 31182)\t1\n",
      "  (0, 63930)\t1\n",
      "  (0, 26959)\t1\n",
      "  (0, 4887)\t1\n",
      "  (0, 72557)\t2\n",
      "  (0, 50236)\t1\n",
      "  (0, 28314)\t1\n",
      "  (0, 22737)\t1\n",
      "  (0, 65368)\t1\n",
      "  (0, 68377)\t1\n",
      "  :\t:\n",
      "  (24999, 38755)\t1\n",
      "  (24999, 74324)\t1\n",
      "  (24999, 61380)\t1\n",
      "  (24999, 9881)\t2\n",
      "  (24999, 4124)\t1\n",
      "  (24999, 24125)\t1\n",
      "  (24999, 1277)\t1\n",
      "  (24999, 46680)\t3\n",
      "  (24999, 34683)\t6\n",
      "  (24999, 72753)\t2\n",
      "  (24999, 34585)\t4\n",
      "  (24999, 30211)\t1\n",
      "  (24999, 44639)\t2\n",
      "  (24999, 70331)\t1\n",
      "  (24999, 72196)\t2\n",
      "  (24999, 34255)\t1\n",
      "  (24999, 35787)\t1\n",
      "  (24999, 41519)\t1\n",
      "  (24999, 2148)\t1\n",
      "  (24999, 3258)\t14\n",
      "  (24999, 31095)\t1\n",
      "  (24999, 70920)\t1\n",
      "  (24999, 66339)\t8\n",
      "  (24999, 66562)\t6\n",
      "  (24999, 73342)\t1\n"
     ]
    }
   ],
   "source": [
    "print(X_train_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def change_senti_to_list(sentiment):\n",
    "    senti_list = []    \n",
    "    for i in sentiment:\n",
    "        senti_list.append(sentiment[i])\n",
    "    return senti_list       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "senti_vec = change_senti_to_list(train['sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a vocalbulary set for dataset\n",
    "def createVocabList(dataSet):\n",
    "    vocabSet = set([])\n",
    "    for document in dataSet:\n",
    "        vocabSet = vocabSet | set(document)#\n",
    "    return list(vocabSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['closeted',\n",
       " 'masturbatory',\n",
       " 'embellish',\n",
       " 'bullhorn',\n",
       " 'counselors',\n",
       " 'gandofini',\n",
       " 'voices',\n",
       " 'mustangs',\n",
       " 'overbaked',\n",
       " 'lest',\n",
       " 'scarfs',\n",
       " 'condescendingly',\n",
       " 'mcvay',\n",
       " 'shian',\n",
       " 'speirs',\n",
       " 'projected',\n",
       " 'upendra',\n",
       " 'totemic',\n",
       " 'carrott',\n",
       " 'bejeebers',\n",
       " 'galatea',\n",
       " 'busch',\n",
       " 'rimi',\n",
       " 'ocron',\n",
       " 'pricey',\n",
       " 'korean',\n",
       " 'syncing',\n",
       " 'philosophized',\n",
       " 'storytellers',\n",
       " 'suman',\n",
       " 'ifan',\n",
       " 'villiers',\n",
       " 'rhind',\n",
       " 'conversion',\n",
       " 'sabato',\n",
       " 'investogate',\n",
       " 'boskovich',\n",
       " 'repercussion',\n",
       " 'berr',\n",
       " 'popularism',\n",
       " 'wordsmith',\n",
       " 'jagged',\n",
       " 'ariauna',\n",
       " 'madhouse',\n",
       " 'dixon',\n",
       " 'evanescence',\n",
       " 'hippiest',\n",
       " 'butcher',\n",
       " 'breathless',\n",
       " 'kissing',\n",
       " 'rockumentary',\n",
       " 'outlandishness',\n",
       " 'tographers',\n",
       " 'wino',\n",
       " 'dancin',\n",
       " 'uktv',\n",
       " 'quadrilateral',\n",
       " 'declan',\n",
       " 'ruefully',\n",
       " 'sitcoms',\n",
       " 'tvg',\n",
       " 'informed',\n",
       " 'lattes',\n",
       " 'unpretentious',\n",
       " 'wowzors',\n",
       " 'brutality',\n",
       " 'windlass',\n",
       " 'ceiling',\n",
       " 'borrows',\n",
       " 'childbirth',\n",
       " 'oppose',\n",
       " 'marm',\n",
       " 'scanningcelebrity',\n",
       " 'ritin',\n",
       " 'scrawl',\n",
       " 'precipitate',\n",
       " 'gibb',\n",
       " 'purchasing',\n",
       " 'chretien',\n",
       " 'spreadeagled',\n",
       " 'liszt',\n",
       " 'dividing',\n",
       " 'freakish',\n",
       " 'sweeny',\n",
       " 'cradled',\n",
       " 'lazarushian',\n",
       " 'tag',\n",
       " 'visited',\n",
       " 'bopping',\n",
       " 'romy',\n",
       " 'dressers',\n",
       " 'overactive',\n",
       " 'psilcybe',\n",
       " 'abhimaan',\n",
       " 'benefit',\n",
       " 'erkia',\n",
       " 'stroup',\n",
       " 'mammothly',\n",
       " 'baldly',\n",
       " 'dissatisfying',\n",
       " 'shepherds',\n",
       " 'boles',\n",
       " 'killearn',\n",
       " 'rampling',\n",
       " 'decisively',\n",
       " 'mala',\n",
       " 'elderly',\n",
       " 'heightens',\n",
       " 'jungian',\n",
       " 'spidey',\n",
       " 'shoddiness',\n",
       " 'deadwood',\n",
       " 'krog',\n",
       " 'standout',\n",
       " 'gangmembers',\n",
       " 'cholate',\n",
       " 'substitutions',\n",
       " 'april',\n",
       " 'lesbos',\n",
       " 'ego',\n",
       " 'dutched',\n",
       " 'roslin',\n",
       " 'afterstory',\n",
       " 'bunk',\n",
       " 'debunk',\n",
       " 'insulting',\n",
       " 'plato',\n",
       " 'chimera',\n",
       " 'congradulations',\n",
       " 'thirties',\n",
       " 'chihuahuawoman',\n",
       " 'stressful',\n",
       " 'quivering',\n",
       " 'smaller',\n",
       " 'tailor',\n",
       " 'unattended',\n",
       " 'el',\n",
       " 'libertini',\n",
       " 'epater',\n",
       " 'benetakos',\n",
       " 'unhesitatingly',\n",
       " 'hashing',\n",
       " 'wagon',\n",
       " 'goodrich',\n",
       " 'tl',\n",
       " 'satred',\n",
       " 'campsites',\n",
       " 'shiva',\n",
       " 'indicitive',\n",
       " 'redhead',\n",
       " 'wigs',\n",
       " 'thunderblast',\n",
       " 'express',\n",
       " 'kardos',\n",
       " 'nadji',\n",
       " 'matriculates',\n",
       " 'jaayen',\n",
       " 'bullock',\n",
       " 'limos',\n",
       " 'vault',\n",
       " 'happend',\n",
       " 'lone',\n",
       " 'mog',\n",
       " 'gnarled',\n",
       " 'collapsing',\n",
       " 'crayons',\n",
       " 'freeing',\n",
       " 'almeida',\n",
       " 'persecuting',\n",
       " 'mitropa',\n",
       " 'cheapest',\n",
       " 'woah',\n",
       " 'lovelier',\n",
       " 'boxset',\n",
       " 'hallway',\n",
       " 'indonesians',\n",
       " 'dunaway',\n",
       " 'cripplingly',\n",
       " 'menopuasal',\n",
       " 'fictional',\n",
       " 'declare',\n",
       " 'dialectics',\n",
       " 'boi',\n",
       " 'fmv',\n",
       " 'nair',\n",
       " 'mps',\n",
       " 'dinsey',\n",
       " 'brumes',\n",
       " 'nitrous',\n",
       " 'parlors',\n",
       " 'achile',\n",
       " 'tamako',\n",
       " 'monty',\n",
       " 'playoffs',\n",
       " 'stroud',\n",
       " 'punjabi',\n",
       " 'resurgence',\n",
       " 'unravelling',\n",
       " 'traceys',\n",
       " 'adhd',\n",
       " 'unhappy',\n",
       " 'baap',\n",
       " 'lensed',\n",
       " 'moshana',\n",
       " 'hilda',\n",
       " 'strolling',\n",
       " 'sparklers',\n",
       " 'compulsory',\n",
       " 'vileness',\n",
       " 'treaters',\n",
       " 'hotter',\n",
       " 'ghotst',\n",
       " 'storysgt',\n",
       " 'badanil',\n",
       " 'brokerage',\n",
       " 'bergan',\n",
       " 'oliva',\n",
       " 'wobbling',\n",
       " 'backdoor',\n",
       " 'supertank',\n",
       " 'richert',\n",
       " 'vocals',\n",
       " 'radiate',\n",
       " 'macliammoir',\n",
       " 'infiltrating',\n",
       " 'cristies',\n",
       " 'mountain',\n",
       " 'seagle',\n",
       " 'scoops',\n",
       " 'masterpieces',\n",
       " 'charater',\n",
       " 'doubter',\n",
       " 'britsh',\n",
       " 'hippler',\n",
       " 'aisle',\n",
       " 'compositions',\n",
       " 'floraine',\n",
       " 'gamera',\n",
       " 'contracted',\n",
       " 'swindlers',\n",
       " 'warnning',\n",
       " 'culminates',\n",
       " 'cordova',\n",
       " 'tideland',\n",
       " 'mochrie',\n",
       " 'chokeslammed',\n",
       " 'congregates',\n",
       " 'washburn',\n",
       " 'entreat',\n",
       " 'subscribes',\n",
       " 'amu',\n",
       " 'provocations',\n",
       " 'whpat',\n",
       " 'seens',\n",
       " 'moonwalk',\n",
       " 'turaqistan',\n",
       " 'inverting',\n",
       " 'newspeak',\n",
       " 'hushed',\n",
       " 'potemkin',\n",
       " 'selbst',\n",
       " 'anticipatory',\n",
       " 'layouts',\n",
       " 'gruesomeness',\n",
       " 'luxuriant',\n",
       " 'asap',\n",
       " 'strictness',\n",
       " 'duplicitous',\n",
       " 'effigies',\n",
       " 'dangle',\n",
       " 'unisten',\n",
       " 'buys',\n",
       " 'unconfortable',\n",
       " 'cabs',\n",
       " 'gifted',\n",
       " 'riot',\n",
       " 'lists',\n",
       " 'fowl',\n",
       " 'trickle',\n",
       " 'orator',\n",
       " 'usurper',\n",
       " 'inseparability',\n",
       " 'nikolai',\n",
       " 'odbray',\n",
       " 'futterman',\n",
       " 'esoteric',\n",
       " 'ecclesiastical',\n",
       " 'sink',\n",
       " 'kibbutznikim',\n",
       " 'bulk',\n",
       " 'hurtful',\n",
       " 'candleshoe',\n",
       " 'mattie',\n",
       " 'clarity',\n",
       " 'garnished',\n",
       " 'tsukurou',\n",
       " 'auberjonois',\n",
       " 'deletion',\n",
       " 'yamamura',\n",
       " 'rajendra',\n",
       " 'beswick',\n",
       " 'voodoo',\n",
       " 'graphically',\n",
       " 'rebuttal',\n",
       " 'fowarded',\n",
       " 'schleps',\n",
       " 'generalization',\n",
       " 'motorized',\n",
       " 'spoofs',\n",
       " 'nekkid',\n",
       " 'scout',\n",
       " 'atalante',\n",
       " 'detraction',\n",
       " 'deserts',\n",
       " 'trashbin',\n",
       " 'loans',\n",
       " 'mosley',\n",
       " 'groceries',\n",
       " 'kya',\n",
       " 'angelyne',\n",
       " 'pardons',\n",
       " 'higres',\n",
       " 'borowcyzk',\n",
       " 'plateaus',\n",
       " 'mainsprings',\n",
       " 'stamina',\n",
       " 'dexterous',\n",
       " 'ucm',\n",
       " 'ehh',\n",
       " 'heartbreaker',\n",
       " 'elinore',\n",
       " 'bleeds',\n",
       " 'biology',\n",
       " 'doggerel',\n",
       " 'amalgamated',\n",
       " 'decameron',\n",
       " 'juggler',\n",
       " 'brussel',\n",
       " 'kabala',\n",
       " 'cavalier',\n",
       " 'lifestyles',\n",
       " 'chatter',\n",
       " 'paints',\n",
       " 'tearjerking',\n",
       " 'trevino',\n",
       " 'yeeeowch',\n",
       " 'disorienting',\n",
       " 'bringing',\n",
       " 'reboot',\n",
       " 'nuff',\n",
       " 'eleanore',\n",
       " 'catalyzing',\n",
       " 'plangent',\n",
       " 'hellraiser',\n",
       " 'loffe',\n",
       " 'heres',\n",
       " 'afilm',\n",
       " 'illusionist',\n",
       " 'harbour',\n",
       " 'hearen',\n",
       " 'quotation',\n",
       " 'blamed',\n",
       " 'gatt',\n",
       " 'tigra',\n",
       " 'massy',\n",
       " 'trended',\n",
       " 'armistead',\n",
       " 'mc',\n",
       " 'elapsed',\n",
       " 'seto',\n",
       " 'trilateralists',\n",
       " 'hideously',\n",
       " 'weem',\n",
       " 'kaye',\n",
       " 'pitchers',\n",
       " 'movieee',\n",
       " 'misaki',\n",
       " 'neon',\n",
       " 'postcard',\n",
       " 'hitman',\n",
       " 'merrily',\n",
       " 'brilliantdont',\n",
       " 'paramedics',\n",
       " 'paramore',\n",
       " 'accomplishments',\n",
       " 'delinquency',\n",
       " 'surronding',\n",
       " 'maison',\n",
       " 'lao',\n",
       " 'rippling',\n",
       " 'tolkein',\n",
       " 'rendevous',\n",
       " 'instaneously',\n",
       " 'mellifluousness',\n",
       " 'beauseigneur',\n",
       " 'collaborative',\n",
       " 'delts',\n",
       " 'defecated',\n",
       " 'lethargy',\n",
       " 'smithi',\n",
       " 'erased',\n",
       " 'hustling',\n",
       " 'bauchau',\n",
       " 'nigga',\n",
       " 'yasuzo',\n",
       " 'aleisa',\n",
       " 'annabeth',\n",
       " 'sergej',\n",
       " 'itv',\n",
       " 'celtic',\n",
       " 'edinburugh',\n",
       " 'cus',\n",
       " 'alienating',\n",
       " 'timur',\n",
       " 'grammar',\n",
       " 'beconsidered',\n",
       " 'dabney',\n",
       " 'dekhiye',\n",
       " 'cloth',\n",
       " 'laster',\n",
       " 'enachanted',\n",
       " 'troll',\n",
       " 'buttercream',\n",
       " 'guilliani',\n",
       " 'cascading',\n",
       " 'geese',\n",
       " 'philosophize',\n",
       " 'spoileri',\n",
       " 'mummies',\n",
       " 'ravis',\n",
       " 'indisposed',\n",
       " 'steiner',\n",
       " 'csi',\n",
       " 'puaro',\n",
       " 'googling',\n",
       " 'atavistic',\n",
       " 'heartfelt',\n",
       " 'railway',\n",
       " 'waves',\n",
       " 'prides',\n",
       " 'possession',\n",
       " 'cigarretes',\n",
       " 'evidence',\n",
       " 'clubberin',\n",
       " 'exposition',\n",
       " 'heartwarming',\n",
       " 'richest',\n",
       " 'sondergaard',\n",
       " 'australia',\n",
       " 'cognac',\n",
       " 'teasing',\n",
       " 'tounge',\n",
       " 'sleepapedic',\n",
       " 'prosecutor',\n",
       " 'alerted',\n",
       " 'modem',\n",
       " 'greet',\n",
       " 'firebug',\n",
       " 'hoppers',\n",
       " 'andrew',\n",
       " 'memorably',\n",
       " 'hata',\n",
       " 'plunkett',\n",
       " 'corri',\n",
       " 'naturally',\n",
       " 'humidity',\n",
       " 'mediums',\n",
       " 'programmer',\n",
       " 'libertine',\n",
       " 'greaest',\n",
       " 'vandalised',\n",
       " 'wirth',\n",
       " 'cid',\n",
       " 'dripping',\n",
       " 'withstands',\n",
       " 'advertized',\n",
       " 'meercats',\n",
       " 'dresser',\n",
       " 'condoli',\n",
       " 'protester',\n",
       " 'indicative',\n",
       " 'meanly',\n",
       " 'defunct',\n",
       " 'mongoloid',\n",
       " 'pensacolians',\n",
       " 'shivers',\n",
       " 'astoria',\n",
       " 'adam',\n",
       " 'promicing',\n",
       " 'corinthian',\n",
       " 'frisco',\n",
       " 'reiterating',\n",
       " 'horsies',\n",
       " 'monette',\n",
       " 'branch',\n",
       " 'av',\n",
       " 'fond',\n",
       " 'inheritances',\n",
       " 'debunked',\n",
       " 'dominates',\n",
       " 'ando',\n",
       " 'llbean',\n",
       " 'hammill',\n",
       " 'leg',\n",
       " 'barky',\n",
       " 'latch',\n",
       " 'controversy',\n",
       " 'propositioned',\n",
       " 'deirdre',\n",
       " 'catapult',\n",
       " 'serges',\n",
       " 'divided',\n",
       " 'spriggs',\n",
       " 'assortment',\n",
       " 'surfing',\n",
       " 'vouching',\n",
       " 'kermit',\n",
       " 'annulled',\n",
       " 'stabbing',\n",
       " 'tonne',\n",
       " 'decency',\n",
       " 'filmmmakers',\n",
       " 'rrratman',\n",
       " 'phillipe',\n",
       " 'recap',\n",
       " 'masonite',\n",
       " 'topness',\n",
       " 'kaliganj',\n",
       " 'intros',\n",
       " 'dibler',\n",
       " 'accentuate',\n",
       " 'unmet',\n",
       " 'thesiger',\n",
       " 'lustrous',\n",
       " 'pryce',\n",
       " 'ntr',\n",
       " 'distinct',\n",
       " 'liquer',\n",
       " 'addressing',\n",
       " 'curled',\n",
       " 'rohm',\n",
       " 'ferret',\n",
       " 'podalydes',\n",
       " 'vacillations',\n",
       " 'gaps',\n",
       " 'rotoscoped',\n",
       " 'bhat',\n",
       " 'jz',\n",
       " 'overdramatic',\n",
       " 'woronov',\n",
       " 'glamouresque',\n",
       " 'phoning',\n",
       " 'commencement',\n",
       " 'filmaking',\n",
       " 'langa',\n",
       " 'paraszhanov',\n",
       " 'spout',\n",
       " 'environmentalism',\n",
       " 'succeeded',\n",
       " 'warrick',\n",
       " 'misinterpretated',\n",
       " 'tremell',\n",
       " 'cayman',\n",
       " 'rutting',\n",
       " 'teshigahara',\n",
       " 'lemmya',\n",
       " 'timethis',\n",
       " 'discer',\n",
       " 'belongs',\n",
       " 'emigrates',\n",
       " 'mujhe',\n",
       " 'naoto',\n",
       " 'leander',\n",
       " 'tellegen',\n",
       " 'large',\n",
       " 'lie',\n",
       " 'sinologist',\n",
       " 'maro',\n",
       " 'sydney',\n",
       " 'jackets',\n",
       " 'transvestive',\n",
       " 'innovatory',\n",
       " 'undercover',\n",
       " 'syrkin',\n",
       " 'virgil',\n",
       " 'overreacts',\n",
       " 'constituting',\n",
       " 'enunciated',\n",
       " 'vfx',\n",
       " 'guesses',\n",
       " 'subiaco',\n",
       " 'passersby',\n",
       " 'veeeery',\n",
       " 'prez',\n",
       " 'treck',\n",
       " 'statistically',\n",
       " 'lemarit',\n",
       " 'smashing',\n",
       " 'stevenses',\n",
       " 'torres',\n",
       " 'tam',\n",
       " 'nikko',\n",
       " 'megalomaniacal',\n",
       " 'quence',\n",
       " 'beady',\n",
       " 'nothan',\n",
       " 'pained',\n",
       " 'groping',\n",
       " 'unwelcome',\n",
       " 'cake',\n",
       " 'gavras',\n",
       " 'wachtang',\n",
       " 'divined',\n",
       " 'acus',\n",
       " 'earthbound',\n",
       " 'berdalh',\n",
       " 'airtight',\n",
       " 'pitying',\n",
       " 'kue',\n",
       " 'valderama',\n",
       " 'chipmunk',\n",
       " 'clamshell',\n",
       " 'mssr',\n",
       " 'aveu',\n",
       " 'hive',\n",
       " 'brockie',\n",
       " 'portraited',\n",
       " 'correspond',\n",
       " 'veeeeeeeery',\n",
       " 'founders',\n",
       " 'peacemakers',\n",
       " 'reconcilable',\n",
       " 'marine',\n",
       " 'choi',\n",
       " 'corridors',\n",
       " 'natures',\n",
       " 'ridiculous',\n",
       " 'cleverness',\n",
       " 'lou',\n",
       " 'petrifying',\n",
       " 'bitter',\n",
       " 'download',\n",
       " 'lebanon',\n",
       " 'sprees',\n",
       " 'caballe',\n",
       " 'reigne',\n",
       " 'yilmaz',\n",
       " 'talked',\n",
       " 'batis',\n",
       " 'biospheres',\n",
       " 'inaccurate',\n",
       " 'ish',\n",
       " 'satisying',\n",
       " 'ottawa',\n",
       " 'paradoxical',\n",
       " 'outnumbered',\n",
       " 'cavelleri',\n",
       " 'maiming',\n",
       " 'smugly',\n",
       " 'bough',\n",
       " 'trifecta',\n",
       " 'jakes',\n",
       " 'whelmed',\n",
       " 'mound',\n",
       " 'agitator',\n",
       " 'mouths',\n",
       " 'naushads',\n",
       " 'pabst',\n",
       " 'ota',\n",
       " 'neverland',\n",
       " 'schappert',\n",
       " 'topples',\n",
       " 'sweeping',\n",
       " 'mitali',\n",
       " 'matchpoint',\n",
       " 'strives',\n",
       " 'rhein',\n",
       " 'success',\n",
       " 'phrase',\n",
       " 'molest',\n",
       " 'hoyo',\n",
       " 'classist',\n",
       " 'fringes',\n",
       " 'privacy',\n",
       " 'preamble',\n",
       " 'canoodling',\n",
       " 'snickered',\n",
       " 'lyndon',\n",
       " 'strips',\n",
       " 'grieco',\n",
       " 'hailed',\n",
       " 'mountian',\n",
       " 'centerstage',\n",
       " 'rowan',\n",
       " 'thooughly',\n",
       " 'virginya',\n",
       " 'edda',\n",
       " 'apparition',\n",
       " 'weld',\n",
       " 'intersections',\n",
       " 'chuck',\n",
       " 'humberfloob',\n",
       " 'aeon',\n",
       " 'bigha',\n",
       " 'faints',\n",
       " 'version',\n",
       " 'ransacked',\n",
       " 'celebrating',\n",
       " 'wonderfully',\n",
       " 'pensions',\n",
       " 'fresnay',\n",
       " 'contenders',\n",
       " 'sides',\n",
       " 'wtf',\n",
       " 'yaowwww',\n",
       " 'underserved',\n",
       " 'skerrit',\n",
       " 'ciel',\n",
       " 'shreds',\n",
       " 'backers',\n",
       " 'feore',\n",
       " 'hungrier',\n",
       " 'gangstermovies',\n",
       " 'delusion',\n",
       " 'dupuis',\n",
       " 'norsk',\n",
       " 'widens',\n",
       " 'usurious',\n",
       " 'banishing',\n",
       " 'donaldson',\n",
       " 'astronomically',\n",
       " 'savings',\n",
       " 'farr',\n",
       " 'bonham',\n",
       " 'iffr',\n",
       " 'legros',\n",
       " 'crims',\n",
       " 'lessen',\n",
       " 'whoppers',\n",
       " 'wrestlers',\n",
       " 'compare',\n",
       " 'tornadoes',\n",
       " 'arirang',\n",
       " 'persist',\n",
       " 'griswolds',\n",
       " 'jerol',\n",
       " 'wippleman',\n",
       " 'lookit',\n",
       " 'tomlinson',\n",
       " 'gadfly',\n",
       " 'durya',\n",
       " 'ametuer',\n",
       " 'bromwich',\n",
       " 'hooray',\n",
       " 'sandman',\n",
       " 'stensgaard',\n",
       " 'clangers',\n",
       " 'emphasises',\n",
       " 'burnett',\n",
       " 'sim',\n",
       " 'inventor',\n",
       " 'mentos',\n",
       " 'peat',\n",
       " 'prominant',\n",
       " 'transpiring',\n",
       " 'adrift',\n",
       " 'mikl',\n",
       " 'reognise',\n",
       " 'toy',\n",
       " 'sabriye',\n",
       " 'panhandlers',\n",
       " 'aubry',\n",
       " 'dramatisations',\n",
       " 'rehan',\n",
       " 'ostensible',\n",
       " 'portico',\n",
       " 'excell',\n",
       " 'ulmer',\n",
       " 'hitchhikes',\n",
       " 'tampon',\n",
       " 'dubiety',\n",
       " 'kingdome',\n",
       " 'pitfall',\n",
       " 'capshaw',\n",
       " 'realisations',\n",
       " 'steroids',\n",
       " 'danny',\n",
       " 'abominator',\n",
       " 'codswallop',\n",
       " 'mullins',\n",
       " 'abroad',\n",
       " 'undergraduate',\n",
       " 'zsa',\n",
       " 'quantity',\n",
       " 'everytown',\n",
       " 'robs',\n",
       " 'timbre',\n",
       " 'ond',\n",
       " 'newsom',\n",
       " 'chopping',\n",
       " 'visage',\n",
       " 'hughie',\n",
       " 'shills',\n",
       " 'lindsey',\n",
       " 'classify',\n",
       " 'ramgopal',\n",
       " 'confounds',\n",
       " 'bailed',\n",
       " 'remnants',\n",
       " 'hrithik',\n",
       " 'devoid',\n",
       " 'cruder',\n",
       " 'labute',\n",
       " 'lectern',\n",
       " 'applecart',\n",
       " 'potion',\n",
       " 'revisioning',\n",
       " 'wildmon',\n",
       " 'comfort',\n",
       " 'virtue',\n",
       " 'lisle',\n",
       " 'tracie',\n",
       " 'fermenting',\n",
       " 'havel',\n",
       " 'wallace',\n",
       " 'fulton',\n",
       " 'controller',\n",
       " 'zeek',\n",
       " 'raping',\n",
       " 'etiquette',\n",
       " 'elia',\n",
       " 'fortunately',\n",
       " 'sucka',\n",
       " 'newsome',\n",
       " 'bonacorsi',\n",
       " 'majorette',\n",
       " 'crops',\n",
       " 'enviable',\n",
       " 'aces',\n",
       " 'alvira',\n",
       " 'gumshoes',\n",
       " 'masseratti',\n",
       " 'vulcans',\n",
       " 'soars',\n",
       " 'thine',\n",
       " 'guested',\n",
       " 'theodor',\n",
       " 'gospel',\n",
       " 'hypocritic',\n",
       " 'mammaries',\n",
       " 'espy',\n",
       " 'gullible',\n",
       " 'nieztsche',\n",
       " 'personality',\n",
       " 'gazarra',\n",
       " 'treks',\n",
       " 'nz',\n",
       " 'considered',\n",
       " 'restore',\n",
       " 'fortuitous',\n",
       " 'wishful',\n",
       " 'slope',\n",
       " 'mown',\n",
       " 'race',\n",
       " 'inuindo',\n",
       " 'predicate',\n",
       " 'stadvec',\n",
       " 'dementedly',\n",
       " 'sevalas',\n",
       " 'thundering',\n",
       " 'priveghi',\n",
       " 'excercise',\n",
       " 'menus',\n",
       " 'bwainn',\n",
       " 'boycott',\n",
       " 'murdock',\n",
       " 'loogies',\n",
       " 'sexually',\n",
       " 'woolf',\n",
       " 'wrongdoings',\n",
       " 'arly',\n",
       " 'bouchey',\n",
       " 'pressurized',\n",
       " 'unspenseful',\n",
       " 'dosen',\n",
       " 'hoo',\n",
       " 'sato',\n",
       " 'nissan',\n",
       " 'unresolved',\n",
       " 'enthusiastic',\n",
       " 'spoilerunderrated',\n",
       " 'consensus',\n",
       " 'ruban',\n",
       " 'bucatinsky',\n",
       " 'knuckle',\n",
       " 'eat',\n",
       " 'inca',\n",
       " 'pharaoh',\n",
       " 'incompetente',\n",
       " 'barlow',\n",
       " 'thea',\n",
       " 'culpability',\n",
       " 'pompous',\n",
       " 'aligned',\n",
       " 'mifume',\n",
       " 'teil',\n",
       " 'springboard',\n",
       " 'brujas',\n",
       " 'gbs',\n",
       " 'summerson',\n",
       " 'mourn',\n",
       " 'schrieber',\n",
       " 'cossimo',\n",
       " 'goes',\n",
       " 'treat',\n",
       " 'grauens',\n",
       " 'chapterplays',\n",
       " 'wyeth',\n",
       " 'hoofs',\n",
       " 'carmine',\n",
       " 'pomp',\n",
       " 'cork',\n",
       " 'goofy',\n",
       " 'underlings',\n",
       " 'annoyingly',\n",
       " 'descension',\n",
       " 'religulous',\n",
       " 'resorted',\n",
       " 'vhala',\n",
       " 'entereth',\n",
       " 'steer',\n",
       " 'bolkan',\n",
       " 'parades',\n",
       " 'orangutans',\n",
       " 'hel',\n",
       " 'rvds',\n",
       " 'halls',\n",
       " 'youki',\n",
       " 'theat',\n",
       " 'catering',\n",
       " 'unenjoyable',\n",
       " 'imaginaire',\n",
       " 'spend',\n",
       " 'loch',\n",
       " 'tyranosaurous',\n",
       " 'croat',\n",
       " 'odysseys',\n",
       " 'imposed',\n",
       " 'sulia',\n",
       " 'powerful',\n",
       " 'disallows',\n",
       " 'thongs',\n",
       " 'spinner',\n",
       " 'geurilla',\n",
       " 'hills',\n",
       " 'suppress',\n",
       " 'beluschi',\n",
       " 'sostrong',\n",
       " 'roadie',\n",
       " 'englanders',\n",
       " 'koyi',\n",
       " 'uninviting',\n",
       " 'ataque',\n",
       " 'burry',\n",
       " 'accustomed',\n",
       " 'commando',\n",
       " 'ewww',\n",
       " 'overtops',\n",
       " 'orson',\n",
       " 'outr',\n",
       " 'nooks',\n",
       " 'anachronism',\n",
       " 'urinating',\n",
       " 'carried',\n",
       " 'berg',\n",
       " 'leticia',\n",
       " 'manliness',\n",
       " 'demon',\n",
       " 'bujeau',\n",
       " 'unto',\n",
       " 'perros',\n",
       " 'liferaft',\n",
       " 'holmfrid',\n",
       " 'touchstones',\n",
       " 'balta',\n",
       " 'wormed',\n",
       " 'angrezon',\n",
       " 'probabilities',\n",
       " 'substories',\n",
       " 'honourable',\n",
       " 'camping',\n",
       " 'carpenters',\n",
       " 'unloving',\n",
       " 'meadows',\n",
       " 'befalls',\n",
       " 'thornberrys',\n",
       " 'duddley',\n",
       " 'detectable',\n",
       " 'lamest',\n",
       " 'hanneke',\n",
       " ...]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_list = createVocabList(review_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"\\\\\"The Classic War of the Worlds\\\\\" by Timothy Hines is a very entertaining film that obviously goes to great effort and lengths to faithfully recreate H. G. Wells\\' classic book. Mr. Hines succeeds in doing so. I, and those who watched his film with me, appreciated the fact that it was not the standard, predictable Hollywood fare that comes out every year, e.g. the Spielberg version with Tom Cruise that had only the slightest resemblance to the book. Obviously, everyone looks for different things in a movie. Those who envision themselves as amateur \\\\\"critics\\\\\" look only to criticize everything they can. Others rate a movie on more important bases,like being entertained, which is why most people never agree with the \\\\\"critics\\\\\". We enjoyed the effort Mr. Hines put into being faithful to H.G. Wells\\' classic novel, and we found it to be very entertaining. This made it easy to overlook what the \\\\\"critics\\\\\" perceive to be its shortcomings.\"'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['review'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def setOfWord2Vec(vocabList, inputSet):\n",
    "\tvec = [0] * len(vocabList)\n",
    "\tfor w in inputSet:\n",
    "\t\tif w in vocabList:\n",
    "\t\t\tvec[vocabList.index(w)] = 1\n",
    "\t\telse:\n",
    "\t\t\tprint (\"word: %s is not in my vocabulary!\" % w)\n",
    "\treturn vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = np.random.randint(5, size=(6, 100))\n",
    "y = np.array([1, 2, 3, 4, 5, 6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = MultinomialNB()\n",
    "clf.fit(X, y)\n",
    "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
    "print(clf.predict(X[2:3]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "categories = ['alt.atheism', 'soc.religion.christian',\n",
    "               'comp.graphics', 'sci.med']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "twenty_train = fetch_20newsgroups(subset='train',\n",
    "    categories=categories, shuffle=True, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism', 'comp.graphics', 'sci.med', 'soc.religion.christian']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twenty_train.target_names#y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(twenty_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2257"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(twenty_train.filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4690"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(twenty_train.data)\n",
    "X_train_counts.shape\n",
    "count_vect.vocabulary_.get(u'algorithm')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2257, 35788)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tf_transformer = TfidfTransformer(use_idf=False).fit(X_train_counts)\n",
    "X_train_tf = tf_transformer.transform(X_train_counts)\n",
    "X_train_tf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2257, 35788)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2257"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(twenty_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 14887)\t0.0167978060212\n",
      "  (0, 29022)\t0.13487105543\n",
      "  (0, 8696)\t0.314400065529\n",
      "  (0, 4017)\t0.124918175851\n",
      "  (0, 33256)\t0.118197024901\n",
      "  (0, 21661)\t0.196227989233\n",
      "  (0, 9031)\t0.384180393587\n",
      "  (0, 31077)\t0.0167978060212\n",
      "  (0, 9805)\t0.215672059147\n",
      "  (0, 17366)\t0.0744441018789\n",
      "  (0, 32493)\t0.0728377394162\n",
      "  (0, 16916)\t0.173584720477\n",
      "  (0, 19780)\t0.246455407094\n",
      "  (0, 17302)\t0.186260151092\n",
      "  (0, 23122)\t0.0363749163623\n",
      "  (0, 25663)\t0.0342907063629\n",
      "  (0, 16881)\t0.0360441471878\n",
      "  (0, 16082)\t0.113827386095\n",
      "  (0, 23915)\t0.0177623185636\n",
      "  (0, 32142)\t0.0886541625372\n",
      "  (0, 33597)\t0.0656757804319\n",
      "  (0, 20253)\t0.0168648929771\n",
      "  (0, 587)\t0.0596616201287\n",
      "  (0, 12051)\t0.037793189756\n",
      "  (0, 5201)\t0.0431619970071\n",
      "  :\t:\n",
      "  (2256, 13740)\t0.0850334897249\n",
      "  (2256, 14662)\t0.106756456001\n",
      "  (2256, 20201)\t0.0738057220661\n",
      "  (2256, 12443)\t0.553384865607\n",
      "  (2256, 30325)\t0.285162999154\n",
      "  (2256, 4610)\t0.0927607242625\n",
      "  (2256, 33844)\t0.0984135258166\n",
      "  (2256, 17354)\t0.102560371229\n",
      "  (2256, 26998)\t0.101649875336\n",
      "  (2256, 20277)\t0.101649875336\n",
      "  (2256, 20695)\t0.102560371229\n",
      "  (2256, 20702)\t0.0836931719065\n",
      "  (2256, 9649)\t0.0991689920932\n",
      "  (2256, 9086)\t0.105610847026\n",
      "  (2256, 26254)\t0.0933069451509\n",
      "  (2256, 17133)\t0.196827051633\n",
      "  (2256, 4490)\t0.113953777211\n",
      "  (2256, 13720)\t0.0969927054646\n",
      "  (2256, 5016)\t0.123021329567\n",
      "  (2256, 9632)\t0.113953777211\n",
      "  (2256, 11824)\t0.120285035037\n",
      "  (2256, 29993)\t0.123021329567\n",
      "  (2256, 1298)\t0.126257679086\n",
      "  (2256, 2375)\t0.126257679086\n",
      "  (2256, 3921)\t0.135325231442\n"
     ]
    }
   ],
   "source": [
    "print(X_train_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'God is love' => soc.religion.christian\n",
      "'OpenGL on the GPU is fast' => comp.graphics\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB().fit(X_train_tfidf, twenty_train.target)\n",
    "docs_new = ['God is love', 'OpenGL on the GPU is fast']\n",
    "X_new_counts = count_vect.transform(docs_new)\n",
    "X_new_tfidf = tfidf_transformer.transform(X_new_counts)\n",
    "\n",
    "predicted = clf.predict(X_new_tfidf)\n",
    "\n",
    "\n",
    "\n",
    "for doc, category in zip(docs_new, predicted):\n",
    "    print('%r => %s' % (doc, twenty_train.target_names[category]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip...inear_tf=False, use_idf=True)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "text_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', MultinomialNB()),\n",
    "])\n",
    "text_clf.fit(twenty_train.data, twenty_train.target)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9127829560585885"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "twenty_test = fetch_20newsgroups(subset='test',\n",
    "    categories=categories, shuffle=True, random_state=42)\n",
    "docs_test = twenty_test.data\n",
    "predicted = text_clf.predict(docs_test)\n",
    "np.mean(predicted == twenty_test.target)            \n",
    "from sklearn.linear_model import SGDClassifier\n",
    "text_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', SGDClassifier(loss='hinge', penalty='l2',\n",
    "                                           alpha=1e-3, n_iter=5, random_state=42)),\n",
    "])\n",
    "text_clf.fit(twenty_train.data, twenty_train.target)  \n",
    "\n",
    "predicted = text_clf.predict(docs_test)\n",
    "np.mean(predicted == twenty_test.target)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(docs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(twenty_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        precision    recall  f1-score   support\n",
      "\n",
      "           alt.atheism       0.95      0.81      0.87       319\n",
      "         comp.graphics       0.88      0.97      0.92       389\n",
      "               sci.med       0.94      0.90      0.92       396\n",
      "soc.religion.christian       0.90      0.95      0.93       398\n",
      "\n",
      "           avg / total       0.92      0.91      0.91      1502\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[258,  11,  15,  35],\n",
       "       [  4, 379,   3,   3],\n",
       "       [  5,  33, 355,   3],\n",
       "       [  5,  10,   4, 379]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print(metrics.classification_report(twenty_test.target, predicted,\n",
    "    target_names=twenty_test.target_names))\n",
    "                                        \n",
    "metrics.confusion_matrix(twenty_test.target, predicted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism', 'comp.graphics', 'sci.med', 'soc.religion.christian']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twenty_test.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
